{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full dataset and display it\n",
    "dataset = fetch_covtype(shuffle=True) # make sure to shuffle the data so no local patterns emerge\n",
    "feature_names = dataset.feature_names\n",
    "target_names = dataset.target_names\n",
    "data = dataset.data\n",
    "target = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area_0', 'Wilderness_Area_1', 'Wilderness_Area_2', 'Wilderness_Area_3', 'Soil_Type_0', 'Soil_Type_1', 'Soil_Type_2', 'Soil_Type_3', 'Soil_Type_4', 'Soil_Type_5', 'Soil_Type_6', 'Soil_Type_7', 'Soil_Type_8', 'Soil_Type_9', 'Soil_Type_10', 'Soil_Type_11', 'Soil_Type_12', 'Soil_Type_13', 'Soil_Type_14', 'Soil_Type_15', 'Soil_Type_16', 'Soil_Type_17', 'Soil_Type_18', 'Soil_Type_19', 'Soil_Type_20', 'Soil_Type_21', 'Soil_Type_22', 'Soil_Type_23', 'Soil_Type_24', 'Soil_Type_25', 'Soil_Type_26', 'Soil_Type_27', 'Soil_Type_28', 'Soil_Type_29', 'Soil_Type_30', 'Soil_Type_31', 'Soil_Type_32', 'Soil_Type_33', 'Soil_Type_34', 'Soil_Type_35', 'Soil_Type_36', 'Soil_Type_37', 'Soil_Type_38', 'Soil_Type_39']\n",
      "Target names: ['Cover_Type']\n",
      "Features:  (581012, 54) float64\n",
      "Classes: (581012,) int32\n",
      "Features:  [[2.747e+03 3.200e+01 9.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.711e+03 2.800e+02 1.500e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [3.207e+03 8.000e+01 5.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " ...\n",
      " [3.326e+03 3.430e+02 2.100e+01 ... 0.000e+00 1.000e+00 0.000e+00]\n",
      " [3.115e+03 2.160e+02 8.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [3.270e+03 3.700e+01 9.000e+00 ... 1.000e+00 0.000e+00 0.000e+00]]\n",
      "Classes: [2 2 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature names:\", feature_names)\n",
    "print(\"Target names:\", target_names)\n",
    "print(\"Features: \", data.shape, data.dtype)\n",
    "print(\"Classes:\", target.shape, target.dtype)\n",
    "\n",
    "print(\"Features: \", data)\n",
    "print(\"Classes:\", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline performance is around 95% accuracy. Let's see how close we can get to it. Beware, we are trying to classify using nearest neighbor algorithms instead of the typical logistic regression classification. This might produce better results or worse results. We will explore it soon and see the results for ourselves. There are a lot of dimensions and the dataset is large so anything can happen. Additionally, we also shuffle the dataset so we can compute a more harsher and realistic score later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (389278, 54) (389278,)\n",
      "Test: (191734, 54) (191734,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=1)\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Test:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we split the data to train and test data so it becomes a fair way to judge the model on the test data. We now only use the training data for the nearest neighbor algorithm. Here we do a 33% test data split and 66% train data split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way nearest neighbor works in the most basic case is to find the closest point coressponding to the point we want to classify. This is done with a distance function that we can choose. Two of the most common distances we can choose is manhattan distance or the well known euclidean distance. Let's try euclidean distance since it's more generalized and accurate and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_point(x, y, point):\n",
    "    point = point.reshape((1,-1))\n",
    "    x = x - point # mxn\n",
    "    x2 = x**2 # squared euclidean distances\n",
    "    dist_squared = np.sum(x2,axis=1) # add up all the squared euclidean distances \n",
    "    i = np.argmin(dist_squared)\n",
    "    return y[i] # return the class associated with the closest point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're just predicting a single point and returning a single class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_points(x, y, points):\n",
    "    # same thing as predict_point but we handle multiple points now\n",
    "    mp = points.shape[0]\n",
    "    categories = np.zeros((mp))\n",
    "    for i in range(mp):\n",
    "        categories[i] = predict_point(x,y,points[i]) # use predict_point we defined\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = predict_points(X_train, y_train, X_test) # really slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we predict the points using a loop like this, it becomes really slow. Firstly, there are a 100K data points and additionally for each data point we are getting distances \n",
    "to all the other 400K labels in order to find the closest point. Therefore, it's really inefficient and time complexity is too high. Around 400K*100K or 40 billion operations just for prediction. Therefore, let's just use 100 points and see what the accuracy is for closest point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions [1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 1. 6. 1. 3. 4. 2. 2. 1. 2. 1. 7. 2. 2. 2.\n",
      " 2. 1. 2. 2. 1. 2. 2. 2. 3. 1. 2. 1. 2. 4. 3. 1. 1. 1. 2. 7. 1. 1. 1. 1.\n",
      " 2. 2. 1. 2. 6. 7. 2. 4. 1. 3. 3. 2. 7. 2. 6. 2. 2. 3. 3. 2. 7. 2. 1. 2.\n",
      " 2. 2. 2. 2. 1. 1. 3. 2. 2. 2. 1. 3. 6. 2. 2. 2. 7. 5. 2. 2. 2. 2. 7. 2.\n",
      " 2. 1. 1. 2.]\n",
      "Accuracy: 95.0 %\n"
     ]
    }
   ],
   "source": [
    "samples = 100\n",
    "predictions = predict_points(X_train, y_train, X_test[:samples]) # try only 100 points from the testing set\n",
    "accuracy = accuracy_score(predictions, y_test[:samples]) # compute the accuracy\n",
    "print(\"Accuracy:\",accuracy*100,\"%\") # show the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, we have ~95% accuracy in predicting the labels. Since the baseline is also around 95%, we are actually doing surprisingly well. Of course, this is only for 100 points and for true classification we need to predict the entire test set. But with data shuffling and a good set of points, it's safe to say that simple nearest can perform well in certain situations like this. It seems that the categories are linearly separable and far from each other. Hence, we get a high accuracy like this. But it's important to recognize that performance can be really poor when dealing with high dimensional and high training data. Additionally, for simple classifier we need a lot of training data which we clearly have here. These are some other reasons why logisitic regression is used instead of nearest neighbor algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full dataset and display it\n",
    "dataset = fetch_covtype(shuffle=True) # make sure to shuffle the data so no local patterns emerge\n",
    "feature_names = dataset.feature_names\n",
    "target_names = dataset.target_names\n",
    "data = dataset.data\n",
    "target = dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now knn algorithm is very good. It ignores many outliers if k is chosen to be appropriate and does a good job. However, we have another problem. Suppose we have 10 points. And also assume that we have 4 points for class 1 and 6 points for class 0. If we choose k to be 10, then knn algo will always choose class 0 as the category since there are more points compared to the other class. But what if class 1 was really close to our point we're trying to classify? No matter how close class 1 points get and how far class 0 points stray from, knn will still always pick class 0. This is a flaw in the knn algorithm. This happens because even though we use distance as a metric, once we choose k points, we completely ignore how the distances rank inside of those k points we picked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This motivates the need for a weighted knn algorithm. The idea is very similar to knn. We still pick k nearest points to our point, but instead of picking the most common category, we assign a weight to each of the points based on the distance it's from our point. Closer distances should rank higher and farther distances should rank lower. Therefore weight should be inversely proportional to the distance. A classic function is that of $w = 1/d$, where d is the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (389278, 54) (389278,)\n",
      "Test: (191734, 54) (191734,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=1)\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Test:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_point(x, y, point, k):\n",
    "    point = point.reshape((1,-1))\n",
    "    x = x - point # mxn\n",
    "    x2 = x**2 # squared euclidean distances\n",
    "    dist_squared = np.sum(x2,axis=1) # add up all the squared euclidean distances\n",
    "\n",
    "    k_indices = np.argsort(dist_squared)[:k] # get only the indices of the k minimum distances\n",
    "    k_dist_squared = np.take(dist_squared, k_indices) # get the k minimum distances\n",
    "    k_weights = 1 / k_dist_squared # compute 1/d weights\n",
    "    k_categories = np.take(y, k_indices) # get the categories associated with the k indices\n",
    "\n",
    "    weighted_sum = np.bincount(k_categories, weights = k_weights) # add the weights associated with the categories.\n",
    "    category = np.argmax(weighted_sum) # pick the biggest weighted sum \n",
    "\n",
    "    return category # return the class associated with the k closest points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_points(x, y, points, k):\n",
    "    # same thing as predict_point but we handle multiple points now along with k points\n",
    "    mp = points.shape[0]\n",
    "    categories = np.zeros((mp))\n",
    "    for i in range(mp):\n",
    "        categories[i] = predict_point(x,y,points[i],k) # use predict_point we defined\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K values to test: [ 1 13 26 38 50]\n"
     ]
    }
   ],
   "source": [
    "kvalues = np.round(np.linspace(1,50,5)).astype(np.int32)\n",
    "print(\"K values to test:\",kvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k=1 - 91.0%\n",
      "Accuracy for k=13 - 94.0%\n",
      "Accuracy for k=26 - 93.0%\n",
      "Accuracy for k=38 - 93.0%\n",
      "Accuracy for k=50 - 93.0%\n"
     ]
    }
   ],
   "source": [
    "for i in range(kvalues.shape[0]):\n",
    "    samples = 100\n",
    "    predictions = predict_points(X_train, y_train, X_test[:samples], kvalues[i]) # try only 100 points from the testing set with k=100 points now\n",
    "    accuracy = accuracy_score(predictions, y_test[:samples]) # compute the accuracy\n",
    "    print(f\"Accuracy for k={kvalues[i]} - {accuracy*100}%\") # show the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that for bigger k values, we get higher accuracies, and then eventually decreases again as k becomes too large. If you use a very noisy dataset or if the k value is too big, then this weighted knn should produce a much better result. It's important to note that accuracy for normal knn is higher than this one here. This is because the data is condensed and each separate category is spread out from each other. This clear separation of category and the patterns it exhibits make it fine to use normal knn or even simple nearest neighbor. However, we see here that the weighted version actually does do a lot better than the previous knn classifier for higher values of k. Around 9% accuracy increase for k=50. Therefore, choosing a good k becomes much less significant as you're ranking by the distance anyways. This makes it much better as you can avoid testing so many values of k and choose much less values to test instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab has shown that each nearest neighbor classifier has its own pros and cons. You can't blindly pick any particular classifier without really understanding your data. For these labs, we've shown that many uncertain patterns emerge from our data and it turns out that simple nearest neighbor works really well our case. For other cases, you might have to choose knn or even weighted knn. It's important to be diligent with processes and check your data carefully before making an important decision. :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

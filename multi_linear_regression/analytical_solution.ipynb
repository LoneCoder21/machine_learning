{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64 float64\n",
      "(442, 10)\n",
      "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
      "[[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990749\n",
      "  -0.01764613]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06833155\n",
      "  -0.09220405]\n",
      " [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286131\n",
      "  -0.02593034]\n",
      " ...\n",
      " [ 0.04170844  0.05068012 -0.01590626 ... -0.01107952 -0.04688253\n",
      "   0.01549073]\n",
      " [-0.04547248 -0.04464164  0.03906215 ...  0.02655962  0.04452873\n",
      "  -0.02593034]\n",
      " [-0.04547248 -0.04464164 -0.0730303  ... -0.03949338 -0.00422151\n",
      "   0.00306441]]\n"
     ]
    }
   ],
   "source": [
    "diabetes = load_diabetes(scaled=True) # enable Feature Scaling\n",
    "data = diabetes.data # (m x n) (examples x features)\n",
    "\n",
    "severity = diabetes.target.reshape((-1,1)) # output of severity of diabetes or disease progression\n",
    "# mx1\n",
    "\n",
    "print(data.dtype, severity.dtype) # show datatypes of input and output\n",
    "print(data.shape) # show shape of input\n",
    "print(diabetes.feature_names) # show feature names associated with input\n",
    "print(data)# show data itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is show that linear regression can be solved with an analytical solution. This means that instead of doing gradient descent, we just compute a solution directly using math. There are also certain tricks used to simplify the problem a lot. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03807591  0.05068012  0.06169621 ...  0.01990749 -0.01764613\n",
      "   1.        ]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ... -0.06833155 -0.09220405\n",
      "   1.        ]\n",
      " [ 0.08529891  0.05068012  0.04445121 ...  0.00286131 -0.02593034\n",
      "   1.        ]\n",
      " ...\n",
      " [ 0.04170844  0.05068012 -0.01590626 ... -0.04688253  0.01549073\n",
      "   1.        ]\n",
      " [-0.04547248 -0.04464164  0.03906215 ...  0.04452873 -0.02593034\n",
      "   1.        ]\n",
      " [-0.04547248 -0.04464164 -0.0730303  ... -0.00422151  0.00306441\n",
      "   1.        ]]\n"
     ]
    }
   ],
   "source": [
    "onesarray = np.ones((data.shape[0])).reshape((-1,1))\n",
    "data = np.hstack([data, onesarray]) # add bias feature to the data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main tricks used here is to add 1 as a feature to all the data. Instead of having a separate bias term in the equations, if we add 1 as a feature and remove the bias term from the equation, then the weight associated with that 1 feature will become the bias. This is a clever trick used in machine learning and greatly simplies the involved equations while not chaning the functionality of the model at all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,x):\n",
    "    prediction = np.matmul(x, w) # no need to add bias term here\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_cost(w,x,y):\n",
    "    m = x.shape[0] # number of training examples\n",
    "    n = x.shape[1] # number of features\n",
    "    \n",
    "    prediction = np.matmul(x, w) # no need to add bias term here\n",
    "    error = prediction - y\n",
    "    error = error.reshape((-1)) # make it m (1d array) from mx1\n",
    "    \n",
    "    return np.dot(error, error) / (2*m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now don't have a bias term our predict and cost functions become easier to implement. One of the fundamental ideas in calculus optimization is to find wherever the cost function is at minimum, so we set the partial derivatives to become 0. If we use matrix math first however, then this will be simplified heavily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dJ}{dw_j}= \\frac{1}{m}\\sum_{i=1}^{m}(f_w(X^i)-Y^i)X^i_j\n",
    "$$\n",
    "$$\n",
    "f_w(x)=w_1*x_1+w_2*x_2+w_3*x_3+...+w_n*x_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how there's no bias term in the actual prediction function. It's all part of the weights. The goal now is to re-write these equations into matrix form so we can remove the summation term and simplify heavily with just few matrix operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dJ}{dw_j}= \\frac{1}{m}\\sum_{i=1}^{m}(X^i \\cdot w-Y^i)X^i_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dJ}{dw} = \\frac{1}{m}X^T(Xw-Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last equation is a compressed way to represent all the summations. We also take the transpose of X when multiplying by the other matrices. And now for the calculus, we set the partial derivatives to 0, and do matrix manipulations to find the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dJ}{dw} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{1}{m}X^T(Xw-Y) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "X^T(Xw-Y) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "X^TXw-X^TY = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "X^TXw=X^TY\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to isolate w, we multiply each side by the inverse of the left matrices together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\cancel{(X^TX)^{-1}X^TX}w=(X^TX)^{-1}X^TY\n",
    "$$\n",
    "\n",
    "$$\n",
    "w=(X^TX)^{-1}X^TY\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And boom, we now have isolated the equation in terms of w. We can use this equation to fully solve the linear regression system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_weights(X, Y):\n",
    "    XT = X.T # transpose of X\n",
    "    xtx = np.matmul(XT, X)\n",
    "    xtx_inv = np.linalg.inv(xtx)\n",
    "    inv_xt = np.matmul(xtx_inv, XT)\n",
    "    weights = np.matmul(inv_xt, Y)\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [[ -10.0098663 ]\n",
      " [-239.81564367]\n",
      " [ 519.84592005]\n",
      " [ 324.3846455 ]\n",
      " [-792.17563855]\n",
      " [ 476.73902101]\n",
      " [ 101.04326794]\n",
      " [ 177.06323767]\n",
      " [ 751.27369956]\n",
      " [  67.62669218]\n",
      " [ 152.13348416]]\n"
     ]
    }
   ],
   "source": [
    "w = solve_weights(data, severity)\n",
    "print(\"w:\", w) # weights now include the bias term inside of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 1429.8481737933748\n"
     ]
    }
   ],
   "source": [
    "cost = mse_cost(w,data,severity) # compute new cost based on the weights we computed\n",
    "print(\"Cost:\", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use scikit to verify the solution we just got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Cost: 1438.5400797446343\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDRegressor(max_iter=100, penalty=None)\n",
    "reg = make_pipeline(StandardScaler(), sgd)\n",
    "reg.fit(data, severity.reshape((-1)))\n",
    "sgdout = reg.predict(data).reshape((-1,1))\n",
    "\n",
    "sgderror = (sgdout - severity).reshape((-1)) # compute errors from sgd predictions and actual values\n",
    "sgdcost = np.dot(sgderror, sgderror) / (2*data.shape[0]) # compute minimized final cost\n",
    "print(\"SGD Cost:\", sgdcost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that, very cool. Clearly we are so much better here and the analytical solution does work. This solution is great, but has drawbacks. It's important to only use the analytical solution when the input matrix is small. If the input had many more features then it would take much slower than gradient descent on the order of cubic time. For these large matrices, it's better to use gradient descent. Other than that, if the matrix is small, then we can get away by directly computing the solution. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

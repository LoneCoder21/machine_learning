{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full dataset and display it\n",
    "dataset = load_wine()\n",
    "names = dataset.feature_names\n",
    "data = dataset.data\n",
    "target = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() # feature scale the data first\n",
    "scaler.fit(data)\n",
    "data = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "Data:  (178, 13) float64\n",
      "Target: (178,) int32\n",
      "[[ 1.51861254 -0.5622498   0.23205254 ...  0.36217728  1.84791957\n",
      "   1.01300893]\n",
      " [ 0.24628963 -0.49941338 -0.82799632 ...  0.40605066  1.1134493\n",
      "   0.96524152]\n",
      " [ 0.19687903  0.02123125  1.10933436 ...  0.31830389  0.78858745\n",
      "   1.39514818]\n",
      " ...\n",
      " [ 0.33275817  1.74474449 -0.38935541 ... -1.61212515 -1.48544548\n",
      "   0.28057537]\n",
      " [ 0.20923168  0.22769377  0.01273209 ... -1.56825176 -1.40069891\n",
      "   0.29649784]\n",
      " [ 1.39508604  1.58316512  1.36520822 ... -1.52437837 -1.42894777\n",
      "  -0.59516041]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "target = target\n",
    "\n",
    "print(names)\n",
    "print(\"Data: \", data.shape, data.dtype)\n",
    "print(\"Target:\", target.shape, target.dtype)\n",
    "\n",
    "print(data)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to achieve multi output classification. Since we figured out logistic regression which can so far classify binary labels very well, how can we extend it to multi output? The idea now is to then use multiple binary classification models to simulate the effect of multi output. So yes just by being able to do binary classification we can easily extend it to any n classification categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets: [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0])]\n"
     ]
    }
   ],
   "source": [
    "examples = data.shape[0]\n",
    "features = data.shape[1]\n",
    "categories = np.unique(target).shape[0]\n",
    "models = []\n",
    "binarytargets = []\n",
    "\n",
    "for i in range(categories):\n",
    "    models.append(LogisticRegression(random_state=0))\n",
    "# create as many models as there are categories\n",
    "\n",
    "for i in range(categories):\n",
    "    binarytarget = np.array(target)\n",
    "    binarytarget = (binarytarget ^ i).astype(bool).astype(np.int32)\n",
    "    binarytargets.append(binarytarget)\n",
    "# transform each of the models targets into a binary classification problem\n",
    "\n",
    "print(\"Targets:\", binarytargets)\n",
    "\n",
    "for i in range(categories):\n",
    "    models[i].fit(data, binarytargets[i])\n",
    "# fit each specific target to the associated model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how easy it is to transform the multi classification problem into multiple binary classification problems. The xor checks for the specific ith target and makes it 0 while making the rest of targets non zero. This will return integers and not 1 and 0's so we compare to boolean array first. Then we simply cast the boolean array to an integer 0's and 1's array. With this many arrays corresponding to each category, we can just evaluate the logistic regression model to each category independently and solve the full multi output classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, models):\n",
    "    categories = len(models)\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(categories):\n",
    "        predictions.append(models[i].predict(data))\n",
    "    \n",
    "    pzip = zip(*predictions) # take each binary prediction from separate models and join them in an array\n",
    "    arr = np.array([*pzip]) # take unzipped tuples and put in a 2d array where each row is a sample and each col is the binary prediction\n",
    "    indices = np.argmin(arr,axis=1) # find all zero locations and take the last one\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(data, models)\n",
    "print(\"Predictions:\",predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassifications: 1\n"
     ]
    }
   ],
   "source": [
    "errors = np.sum((target ^ predictions).astype(bool).astype(np.int32))\n",
    "print(\"Misclassifications:\", errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we only have 1 error. So the trick of using multiple models does work in practice albeit it's a bit slow on the otherhand. Let's try to use the generalized multi output logistic regression from scikit instead of using our tricks for verification. This model that scikit uses will actually be proper compared to the hack we just performed. Therefore, it should perform a bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=0)\n",
    "model.fit(data, target)\n",
    "scikit_predictions = model.predict(data) # use scikit multi classification model to predict\n",
    "print(scikit_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassifications: 0\n"
     ]
    }
   ],
   "source": [
    "errors = np.sum((target - scikit_predictions).astype(bool).astype(np.int32))\n",
    "print(\"Misclassifications:\", errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, no errors. Scikit has fully predicted everything with it's complex model. This tells us a few insights. In theory, it's always better to design a proper model. This will lead to better accuracy and is much faster and more numerically stable than the hacks we perform. In reality however, sometimes we have no choice but to use our time wisely. In these situations, it's better to be clever about our approach and we have shown that multi classification is a joke and can easily be done using only binary classification techniques. Therefore, each side has pros and cons and we must make our decision based on them. Even though, I showed all the pros using this hack, there are many cons. The models are going to be explosive in memory and if there are many categories, then it's not viable anymore. Additionally, it's also much slower because each model is trained separately. Finally, there will be accuracy issues for assuming many binary classifications. Good day :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

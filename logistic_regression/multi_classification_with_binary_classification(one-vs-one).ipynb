{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full dataset and display it\n",
    "dataset = load_wine()\n",
    "names = dataset.feature_names\n",
    "data = dataset.data\n",
    "target = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() # feature scale the data first\n",
    "scaler.fit(data)\n",
    "data = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "Data:  (178, 13) float64\n",
      "Target: (178,) int32\n",
      "[[ 1.51861254 -0.5622498   0.23205254 ...  0.36217728  1.84791957\n",
      "   1.01300893]\n",
      " [ 0.24628963 -0.49941338 -0.82799632 ...  0.40605066  1.1134493\n",
      "   0.96524152]\n",
      " [ 0.19687903  0.02123125  1.10933436 ...  0.31830389  0.78858745\n",
      "   1.39514818]\n",
      " ...\n",
      " [ 0.33275817  1.74474449 -0.38935541 ... -1.61212515 -1.48544548\n",
      "   0.28057537]\n",
      " [ 0.20923168  0.22769377  0.01273209 ... -1.56825176 -1.40069891\n",
      "   0.29649784]\n",
      " [ 1.39508604  1.58316512  1.36520822 ... -1.52437837 -1.42894777\n",
      "  -0.59516041]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "target = target\n",
    "\n",
    "print(names)\n",
    "print(\"Data: \", data.shape, data.dtype)\n",
    "print(\"Target:\", target.shape, target.dtype)\n",
    "\n",
    "print(data)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of using one vs rest where we rank one category against the rest, we will rank them in pairs. This will provide better accuracy and the models will be less prone to be biased. This new technique is known as multi class one vs one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits\n",
      "(59, 13)\n",
      "(71, 13)\n",
      "(48, 13)\n"
     ]
    }
   ],
   "source": [
    "examples = data.shape[0]\n",
    "features = data.shape[1]\n",
    "categories = np.unique(target).shape[0]\n",
    "categorysplits = []\n",
    "models = []\n",
    "data_pairs = []\n",
    "target_pairs = []\n",
    "\n",
    "old_ids = 0\n",
    "print(\"Splits\")\n",
    "for i in range(categories):\n",
    "    ids = np.searchsorted(target, i+1)\n",
    "    categorysplits.append(np.array(data[old_ids:ids]))\n",
    "    print(categorysplits[-1].shape)\n",
    "    old_ids = ids\n",
    "# split the categories of data\n",
    "\n",
    "for i in range(categories):\n",
    "    models.append([])\n",
    "    for j in range(categories):\n",
    "        models[-1].append(LogisticRegression(random_state=0))\n",
    "# create a model for each pair of category\n",
    "\n",
    "for i in range(categories):\n",
    "    isize = categorysplits[i].shape[0]\n",
    "    data_pairs.append([])\n",
    "    target_pairs.append([])\n",
    "    for j in range(categories):\n",
    "        jsize = categorysplits[j].shape[0]\n",
    "        joined_categories = np.vstack([categorysplits[i], categorysplits[j]])\n",
    "        joined_targets = np.append(np.zeros(isize),np.ones(jsize)).astype(np.int32)\n",
    "\n",
    "        data_pairs[-1].append(joined_categories)\n",
    "        target_pairs[-1].append(joined_targets)\n",
    "\n",
    "# transform each pair into a binary classification problem\n",
    "for i in range(categories):\n",
    "    for j in range(categories):\n",
    "        models[i][j].fit(data_pairs[i][j], target_pairs[i][j])\n",
    "# fit each category pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're working with much more models here, it's a bit more tricky to organize the data. The idea here is to split the input data by the output categories, then throw away the outputs since we have the input splits. Then we create each category pair of models, create the right preprocessed inputs for a binary classification target and finally use the many models to fit each specific pair. Of course, there's a lot of problems with this approach. Firstly, category A and category B are fit twice, so we're duplicating twice. We could remove this and optimize this to use only half of the pair fits, but it's more annoying to work with that. Also we're fitting category A and category A together, which is useless. Turns out you can ignore this since the probability will be low anyways for the same categories. This can easily be optimized away but I wanted to make the code clean and clear to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, models):\n",
    "    categories = len(models)\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(categories):\n",
    "        predictions.append([])\n",
    "        for j in range(categories):\n",
    "            prediction = models[i][j].predict_proba(data)\n",
    "            prediction = prediction[:,0] # take the class associated with the ith category\n",
    "            predictions[-1].append(prediction)\n",
    "    # get the probabilities for each pair\n",
    "\n",
    "    combined_predictions = []\n",
    "    for i in range(categories):\n",
    "        ipredictions = np.array(predictions[i])\n",
    "        cpredictions = np.prod(ipredictions, axis=0) # take the likelihood of the target category being correct\n",
    "        combined_predictions.append(cpredictions)\n",
    "    # compute the likelihood of the ith category being correct\n",
    "    \n",
    "    pzip = zip(*combined_predictions)\n",
    "    arr = np.array([*pzip]) # reshape the likelihoods into groups of each category\n",
    "    indices = np.argmax(arr,axis=1) #finally take the biggest likelihood index\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction function is very similar to the one versus rest approach. Since we have multiple models, we first predict the probabilities of all of the models on all of the data. Now there are multiple probabilities for each category. To get the joint probability, we can multiply the probabilities together. This is also known as the likelihood of the category being correct. Then, we transform the likelihoods pick the highest likelihood as the category for each training sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(data, models)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassifications: 0\n"
     ]
    }
   ],
   "source": [
    "errors = np.sum((target ^ predictions).astype(bool).astype(np.int32))\n",
    "print(\"Misclassifications:\", errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, look at that. We have no errors. Remember that for one vs rest, we actually got one error. This shows that this technique is a lot more precise than the one vs rest. Of course, with extra precision comes extra cost as well. In one vs rest, we only needed $C$ models for $C$ categories. Here, we needed $C^2$ models since we have pairs of categories being classified together. So if you have too many categories like ~$10^3$, then one vs one is much worse to work with. Of course, in real life we might need to classify a lot more categories such as on the order of million. In this situation, one vs one or even one vs rest wouldn't work as the complexity is too high. To deal with the previous issues, we need to introduce a better technique known as the softmax."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

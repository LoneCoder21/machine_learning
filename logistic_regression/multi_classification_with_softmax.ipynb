{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we arrived to this stage. Previously, we have used the one vs one and one vs rest strategies to convert our multi classification problem to multiple binary classification problems. We also tried nearest neighbor techniques like knn classifier and weighted knn classifier. We have shown that these previous strategies are usually good when used in the proper circumstances. We also showcased many flaws with previous methods. These range from accuracy issues, numeric instability, and performance problems. Today, we will showcase a proper softmax classification method that will solve all of these previous issues if used correctly. We also have shown the softmax derivation to get some intuition in how the loss function will work. Leaving the details till later, we start using some more complicated datasets like handwritten digits. This is known as the Optical recognition of handwritten digits dataset provided by Scikit. With this, we can check how well a basic softmax loss will fare against this dataset. For now, let's start by visualizing some samples of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full dataset and display it\n",
    "dataset = load_digits() # make sure to shuffle the data so no local patterns emerge\n",
    "feature_names = dataset.feature_names\n",
    "target_names = dataset.target_names\n",
    "data = dataset.data\n",
    "target = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature names:\", feature_names)\n",
    "print(\"Target Categories:\", target_names)\n",
    "print(\"Features: \", data.shape, data.dtype)\n",
    "print(\"Classes:\", target.shape, target.dtype)\n",
    "\n",
    "print(\"Features: \", data)\n",
    "print(\"Classes:\", target)\n",
    "print(\"Feature range:\", np.min(data).astype(np.int32), np.max(data).astype(np.int32))\n",
    "print(\"Feature type:\",data.dtype)\n",
    "print(\"Target type:\",target.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8,8,figsize=(6,6))\n",
    "\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        axes[i][j].imshow(data[i*8+j].reshape((8,8)), interpolation='none', cmap=cm.Greys)\n",
    "        axes[i][j].axis('off')\n",
    "plt.suptitle(\"Handwritten digits 8x8 Greyscale (64 images)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 64 images in the dataset. The images are only 8x8 so they're very low quality but it's still very clear to us what each digit the image represents. Let's split the dataset now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=1) # 33 % split\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Test:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scale(data_unscaled, scaled=True):\n",
    "    if scaled == False:\n",
    "        return data_unscaled\n",
    "\n",
    "    max_value = np.max(data_unscaled)\n",
    "\n",
    "    data_scaled = np.array(data_unscaled)\n",
    "    data_scaled = data_scaled / max_value\n",
    "    # min max normalization\n",
    "    \n",
    "    return data_scaled\n",
    "\n",
    "data = feature_scale(data, scaled=True)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we want to feature scale the data in order for gradient descent to perform well. Normally, we would use std normalization as it considers each feature separately and scales the features well. However, here we are dealing with 0-16 values as intensities for the digits. Each feature is a pixel in the image. From this, it's clear that there are many values to represent a specific digit on the image. It could be slanted, rotated, streched, etc. The image however will be completely different even though the digit will be the exact same. So it's ideal to understand that each specific feature is actually useless in the full context of all features. It's the way the different pixels are connected to each other is what matters. With this, the best thing to do is to feature scale the entire image at one by dividing by the full range of the data. This will scale the data between 0 and 1 which will work fine with gradient descent. This is similar to min max normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias_weight(data):\n",
    "    m = data.shape[0]\n",
    "    ones_feature = np.ones(m).reshape((-1,1)) # create a single feature of ones\n",
    "    data_bias = np.hstack([data, ones_feature])\n",
    "    return data_bias\n",
    "\n",
    "data = add_bias_weight(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on out, we need to drastically simplify our equations. Since softmax is an expensive function and we are dealing with multiple weights per category, it helps to simplify the model as much as possible. It turns out that to simplify we can actually remove the bias term from the equations and put it in the weights instead. The feature associated with the bias weight all become 1. This is a small trick to still keep the bias in the model, and it simplifies the model quite a bit. We can now remove the bias term completely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ez = np.exp(x)\n",
    "    return ez / np.sum(ez, axis=1).reshape((-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction will now go through the softmax function. Remember that the prediction now gives a vector of probabilities for each example. If we consider over all examples, we now get a matrix $m \\times k$ where $m$ is the training examples and $k$ is the categories. The softmax is defined as $a_j = \\frac{e^{z_j}}{ \\sum_{k=1}^{N}{e^{z_k} }} $. We just perform this over each example and each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, x, epsilon=1e-15):\n",
    "    prediction = np.matmul(x, w) # m k\n",
    "    prediction = np.clip(prediction, epsilon, 1 - epsilon) # clip to avoid overflow issues\n",
    "    return softmax(prediction) # pass it through a softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also clip the prediction to avoid overflow issues. This is also something we did before passing the value in the sigmoid. Same pattern repeats here. The cost function here is very similar to logistic loss. In logistic loss, we used $-log(p1)$ to compute the loss for probability $p1$. Since there are multiple probabilities, we need weights for each probability value. The loss becomes \n",
    "\\begin{equation}\n",
    "  L(\\mathbf{p},y)=\\begin{cases}\n",
    "    -log(p_1), & \\text{if $y=1$}.\\\\\n",
    "        &\\vdots\\\\\n",
    "     -log(p_k), & \\text{if $y=k$}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "Then the full cost function becomes\n",
    "$$\n",
    "J(\\mathbf{w},b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{k}  1\\left\\{y^{(i)} == j\\right\\} \\log \\frac{e^{z^{(i)}_j}}{\\sum_{p=1}^k e^{z^{(i)}_p} }\n",
    "$$\n",
    "The $1\\left\\{y^{(i)} == j\\right\\}$ is an indicator function where if $y^{(i)} == j$ then the expression becomes a $1$ or $0$ otherwise. Of course, we want to fully vectorize the cost function and the gradients for softmax. This is hard with the indicator function. Instead of using the indicator function we use a one hot encoded matrix $(O_{ij})$ to achieve the same output. We one hot the target labels to get the one hot matrix. Then, the new cost function simplifies to $$J(\\mathbf{w},b) = -\\frac{1}{m}  \\sum_{i=1}^{m} \\sum_{j=1}^{k}  O_{ij} \\log \\frac{e^{z^{(i)}_j}}{\\sum_{p=1}^k e^{z^{(i)}_p} }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_matrix(a):\n",
    "    categories = np.unique(a).shape[0]\n",
    "    return np.eye(categories)[a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that if we use one hot matrix, the code simplifies a lot just like the equation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss_cost(w,x,y):\n",
    "    onehot = onehot_matrix(y) # m k\n",
    "    softmax_prob = predict(w, x) # m k\n",
    "    overk = onehot * np.log(softmax_prob) # m k\n",
    "    overm = np.sum(overk, axis=1) # m\n",
    "    return -np.mean(overm) # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, computing the gradient is tedious as there are many derivatives to take. The final result once we compute the gradient however is very simple. The gradient with respect to a specific weight is $\\frac{\\partial L}{\\partial w_{ji}} = x_j( \\^{y_i} - y_i)$. $i$ is the $i$-th category while $x_j$ refers to the $j$-th feature of the $x$ input vector. $\\^{y_i}$ is the $i$-th predicted probability while $y_i$ is the observed label. This is the gradient of the categorical loss function and it simplifies a lot once you compute it. Of course, this is for a single weight, so we need to apply this over all the weights per each category and over all the training examples to get the average gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(w,x,y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cost_graph(costs, title, color):\n",
    "    iterations = costs.shape[0]\n",
    "    iteration_array = np.arange(0, iterations, dtype=np.int32)\n",
    "    \n",
    "    # graph the cost after updating the model\n",
    "    fig, cost_graph = plt.subplots(layout='constrained')\n",
    "    \n",
    "    cost_graph.set_xlabel(\"Current Iteration\")\n",
    "    cost_graph.set_ylabel(\"Cost\")\n",
    "    \n",
    "    cost_graph.set_title(title)\n",
    "    \n",
    "    cost_graph.plot(iteration_array, costs, color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, gradient_func, cost_func, learning_rate=0.01, max_iterations=1000):\n",
    "    y = y.reshape((-1,1))\n",
    "    m = x.shape[0] # number of training examples\n",
    "    n = x.shape[1] # number of features\n",
    "    k = np.unique(y).shape[0] # number of categories\n",
    "\n",
    "    w = np.zeros((n,k)) # n by k weights\n",
    "    # initialize model parameters to zeroes\n",
    "\n",
    "    costs = np.empty(0)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        dw = gradient_func(w,x,y)\n",
    "        \n",
    "        w -= learning_rate * dw\n",
    "        # update the weights\n",
    "        \n",
    "        current_cost = cost_func(w,x,y)\n",
    "        costs = np.append(costs, current_cost)\n",
    "        # add to array for visualization\n",
    "    return w, costs\n",
    "weights, costs = gradient_descent(data, target, compute_gradients, log_loss_cost, learning_rate=0.1, max_iterations=10000)\n",
    "show_cost_graph(costs, \"Logistic Regression with Softmax on Handwritten Digits\", \"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

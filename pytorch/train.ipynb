{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bdc0473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e8a741",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e38bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b4d77c",
   "metadata": {},
   "source": [
    "**Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5dfd7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75cb02b",
   "metadata": {},
   "source": [
    "**Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d6113ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbdf633",
   "metadata": {},
   "source": [
    "**Loss Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f4ffbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b3483",
   "metadata": {},
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "238552fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352c73b9",
   "metadata": {},
   "source": [
    "**Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc2f1135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3770b5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.300874  [   64/60000]\n",
      "loss: 2.280496  [ 6464/60000]\n",
      "loss: 2.258363  [12864/60000]\n",
      "loss: 2.251217  [19264/60000]\n",
      "loss: 2.225681  [25664/60000]\n",
      "loss: 2.199810  [32064/60000]\n",
      "loss: 2.201290  [38464/60000]\n",
      "loss: 2.169232  [44864/60000]\n",
      "loss: 2.166317  [51264/60000]\n",
      "loss: 2.129716  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 2.123316 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.136759  [   64/60000]\n",
      "loss: 2.124194  [ 6464/60000]\n",
      "loss: 2.063563  [12864/60000]\n",
      "loss: 2.081148  [19264/60000]\n",
      "loss: 2.016143  [25664/60000]\n",
      "loss: 1.959573  [32064/60000]\n",
      "loss: 1.980693  [38464/60000]\n",
      "loss: 1.905674  [44864/60000]\n",
      "loss: 1.909801  [51264/60000]\n",
      "loss: 1.826885  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 1.830790 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.867406  [   64/60000]\n",
      "loss: 1.835014  [ 6464/60000]\n",
      "loss: 1.719348  [12864/60000]\n",
      "loss: 1.760981  [19264/60000]\n",
      "loss: 1.632597  [25664/60000]\n",
      "loss: 1.599447  [32064/60000]\n",
      "loss: 1.613599  [38464/60000]\n",
      "loss: 1.525870  [44864/60000]\n",
      "loss: 1.549298  [51264/60000]\n",
      "loss: 1.442370  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 1.462724 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.532896  [   64/60000]\n",
      "loss: 1.499633  [ 6464/60000]\n",
      "loss: 1.352804  [12864/60000]\n",
      "loss: 1.429715  [19264/60000]\n",
      "loss: 1.295258  [25664/60000]\n",
      "loss: 1.300566  [32064/60000]\n",
      "loss: 1.319475  [38464/60000]\n",
      "loss: 1.250230  [44864/60000]\n",
      "loss: 1.281953  [51264/60000]\n",
      "loss: 1.190886  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.210530 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.287152  [   64/60000]\n",
      "loss: 1.274498  [ 6464/60000]\n",
      "loss: 1.107963  [12864/60000]\n",
      "loss: 1.223630  [19264/60000]\n",
      "loss: 1.085421  [25664/60000]\n",
      "loss: 1.109396  [32064/60000]\n",
      "loss: 1.144617  [38464/60000]\n",
      "loss: 1.083590  [44864/60000]\n",
      "loss: 1.118188  [51264/60000]\n",
      "loss: 1.045505  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 1.057286 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c89b207e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[ 0.0150, -0.0214, -0.0056,  ..., -0.0023, -0.0274, -0.0089],\n",
       "                      [-0.0219,  0.0099,  0.0095,  ...,  0.0182, -0.0191, -0.0221],\n",
       "                      [ 0.0040,  0.0267, -0.0062,  ...,  0.0355,  0.0161, -0.0099],\n",
       "                      ...,\n",
       "                      [ 0.0198, -0.0096, -0.0252,  ...,  0.0021,  0.0077,  0.0293],\n",
       "                      [ 0.0319,  0.0197, -0.0209,  ...,  0.0007,  0.0206,  0.0315],\n",
       "                      [ 0.0331,  0.0182,  0.0047,  ...,  0.0210,  0.0199,  0.0121]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([-0.0054,  0.0400, -0.0267,  0.0154, -0.0265,  0.0216, -0.0236, -0.0296,\n",
       "                       0.0325, -0.0065,  0.0045,  0.0267, -0.0334,  0.0028,  0.0112, -0.0082,\n",
       "                       0.0107,  0.0194, -0.0085, -0.0215,  0.0064,  0.0094, -0.0229,  0.0367,\n",
       "                       0.0419, -0.0332,  0.0072,  0.0259, -0.0314, -0.0245,  0.0076, -0.0149,\n",
       "                       0.0460,  0.0227, -0.0053, -0.0160, -0.0050,  0.0109, -0.0310,  0.0076,\n",
       "                      -0.0239,  0.0113,  0.0256, -0.0359,  0.0121, -0.0179,  0.0082, -0.0162,\n",
       "                      -0.0022, -0.0319,  0.0224,  0.0281,  0.0245, -0.0299,  0.0367,  0.0143,\n",
       "                       0.0303, -0.0252,  0.0084, -0.0190,  0.0156, -0.0188,  0.0011,  0.0124,\n",
       "                       0.0014,  0.0076, -0.0241,  0.0061,  0.0075,  0.0347, -0.0029, -0.0012,\n",
       "                       0.0292, -0.0148, -0.0126, -0.0151, -0.0126,  0.0071,  0.0211,  0.0378,\n",
       "                       0.0052,  0.0304, -0.0057,  0.0022,  0.0049,  0.0154, -0.0178,  0.0010,\n",
       "                      -0.0340,  0.0148, -0.0097, -0.0022, -0.0335, -0.0232,  0.0087, -0.0036,\n",
       "                      -0.0091, -0.0167, -0.0354,  0.0217,  0.0164,  0.0247,  0.0333,  0.0222,\n",
       "                      -0.0273,  0.0209,  0.0224, -0.0029, -0.0300,  0.0371,  0.0243, -0.0139,\n",
       "                       0.0171, -0.0063,  0.0139, -0.0036,  0.0075, -0.0013, -0.0020,  0.0381,\n",
       "                       0.0035, -0.0154,  0.0026,  0.0183,  0.0114,  0.0363,  0.0196, -0.0154,\n",
       "                      -0.0357,  0.0424,  0.0293,  0.0003,  0.0059, -0.0207, -0.0236, -0.0163,\n",
       "                      -0.0339,  0.0043,  0.0229,  0.0397,  0.0099, -0.0134, -0.0032, -0.0258,\n",
       "                       0.0104, -0.0068, -0.0225,  0.0180,  0.0385,  0.0234, -0.0352, -0.0049,\n",
       "                       0.0031, -0.0230,  0.0174, -0.0139, -0.0125,  0.0360, -0.0077,  0.0091,\n",
       "                      -0.0199,  0.0255,  0.0302,  0.0105,  0.0054, -0.0229, -0.0225, -0.0100,\n",
       "                      -0.0344,  0.0010,  0.0130,  0.0070,  0.0296,  0.0216,  0.0368,  0.0250,\n",
       "                      -0.0229,  0.0351, -0.0253,  0.0172, -0.0301,  0.0307, -0.0186, -0.0163,\n",
       "                      -0.0180,  0.0045,  0.0168,  0.0163,  0.0373, -0.0219,  0.0037,  0.0255,\n",
       "                       0.0051,  0.0170,  0.0268,  0.0263,  0.0074,  0.0324,  0.0364,  0.0274,\n",
       "                      -0.0148,  0.0355,  0.0280,  0.0277,  0.0365,  0.0023,  0.0044,  0.0046,\n",
       "                       0.0241, -0.0075, -0.0029,  0.0085, -0.0157, -0.0285,  0.0150,  0.0314,\n",
       "                      -0.0190, -0.0292,  0.0118,  0.0193,  0.0223,  0.0475,  0.0455, -0.0322,\n",
       "                      -0.0306, -0.0009,  0.0235, -0.0131, -0.0320,  0.0326,  0.0263, -0.0165,\n",
       "                       0.0377,  0.0235,  0.0117,  0.0141,  0.0030,  0.0051,  0.0335, -0.0250,\n",
       "                       0.0089, -0.0100,  0.0180,  0.0035,  0.0100,  0.0269,  0.0023, -0.0199,\n",
       "                       0.0339, -0.0036,  0.0017,  0.0063, -0.0284,  0.0161,  0.0263, -0.0355,\n",
       "                      -0.0280,  0.0192,  0.0073,  0.0065, -0.0214,  0.0203, -0.0218,  0.0099,\n",
       "                       0.0082, -0.0141,  0.0208, -0.0321,  0.0329,  0.0226, -0.0357, -0.0045,\n",
       "                       0.0319,  0.0189,  0.0293, -0.0283, -0.0207,  0.0144,  0.0193,  0.0054,\n",
       "                       0.0031,  0.0118, -0.0029,  0.0054, -0.0261,  0.0065, -0.0105,  0.0316,\n",
       "                      -0.0097, -0.0070, -0.0224, -0.0319,  0.0255, -0.0241,  0.0210,  0.0359,\n",
       "                       0.0301,  0.0349,  0.0227,  0.0308,  0.0150,  0.0045,  0.0212, -0.0297,\n",
       "                      -0.0109, -0.0107, -0.0267,  0.0265, -0.0255,  0.0240, -0.0136,  0.0086,\n",
       "                       0.0106,  0.0018,  0.0175, -0.0215, -0.0244, -0.0017,  0.0063,  0.0180,\n",
       "                      -0.0111,  0.0357,  0.0010, -0.0210, -0.0079,  0.0232, -0.0407, -0.0157,\n",
       "                      -0.0018,  0.0331,  0.0337,  0.0155, -0.0122,  0.0289, -0.0363,  0.0240,\n",
       "                       0.0203, -0.0276,  0.0077,  0.0023,  0.0091, -0.0230,  0.0058,  0.0307,\n",
       "                       0.0041, -0.0051, -0.0142, -0.0347,  0.0174, -0.0314,  0.0178,  0.0170,\n",
       "                      -0.0254, -0.0236, -0.0233,  0.0487, -0.0201, -0.0222,  0.0338, -0.0206,\n",
       "                      -0.0333, -0.0252, -0.0112, -0.0075,  0.0260,  0.0255, -0.0229, -0.0163,\n",
       "                       0.0167,  0.0262,  0.0170,  0.0147, -0.0187,  0.0201,  0.0176,  0.0056,\n",
       "                      -0.0279, -0.0280, -0.0248, -0.0004,  0.0252,  0.0196,  0.0084, -0.0042,\n",
       "                      -0.0154,  0.0363, -0.0025, -0.0136,  0.0044,  0.0057, -0.0374, -0.0160,\n",
       "                      -0.0085,  0.0150,  0.0082,  0.0111, -0.0119, -0.0235,  0.0305,  0.0099,\n",
       "                      -0.0013, -0.0125,  0.0131, -0.0196, -0.0216, -0.0038,  0.0258,  0.0179,\n",
       "                       0.0128,  0.0085,  0.0172, -0.0177, -0.0074, -0.0186,  0.0056,  0.0226,\n",
       "                      -0.0124,  0.0068, -0.0069, -0.0164, -0.0292,  0.0178, -0.0223, -0.0115,\n",
       "                       0.0006,  0.0176, -0.0044,  0.0080,  0.0272,  0.0166,  0.0286,  0.0167,\n",
       "                      -0.0031,  0.0068,  0.0220, -0.0330, -0.0128,  0.0031,  0.0135, -0.0202,\n",
       "                       0.0283,  0.0169, -0.0201,  0.0284,  0.0295, -0.0210, -0.0220,  0.0100,\n",
       "                       0.0231,  0.0339, -0.0168, -0.0136, -0.0319,  0.0039, -0.0065, -0.0118,\n",
       "                      -0.0314, -0.0008, -0.0012, -0.0261,  0.0337, -0.0232, -0.0192, -0.0133,\n",
       "                      -0.0136,  0.0109, -0.0095, -0.0220, -0.0338,  0.0269, -0.0031,  0.0447,\n",
       "                       0.0335, -0.0092,  0.0237, -0.0260,  0.0257,  0.0225, -0.0115, -0.0109,\n",
       "                       0.0371, -0.0174,  0.0352, -0.0037, -0.0266,  0.0011,  0.0401, -0.0055,\n",
       "                       0.0160, -0.0192, -0.0081, -0.0092,  0.0209,  0.0151, -0.0200,  0.0011,\n",
       "                       0.0258, -0.0146,  0.0125, -0.0031,  0.0200, -0.0111,  0.0285,  0.0192,\n",
       "                       0.0330,  0.0052,  0.0229,  0.0181,  0.0331,  0.0163,  0.0087, -0.0009])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[-0.0422, -0.0203,  0.0367,  ..., -0.0152,  0.0198,  0.0336],\n",
       "                      [-0.0180, -0.0103,  0.0296,  ...,  0.0340,  0.0417, -0.0405],\n",
       "                      [-0.0417,  0.0324,  0.0081,  ..., -0.0396, -0.0236, -0.0045],\n",
       "                      ...,\n",
       "                      [ 0.0404, -0.0064, -0.0330,  ...,  0.0051, -0.0268, -0.0148],\n",
       "                      [-0.0087,  0.0056,  0.0364,  ...,  0.0172,  0.0382, -0.0206],\n",
       "                      [ 0.0273,  0.0157,  0.0227,  ..., -0.0120,  0.0185, -0.0212]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([ 3.3960e-02,  3.7976e-02, -5.5131e-03, -5.2285e-03,  6.9481e-03,\n",
       "                      -3.8648e-02,  4.8150e-02,  3.7942e-02, -2.8250e-02,  9.6002e-03,\n",
       "                       2.8774e-02,  2.4381e-02, -3.6476e-02, -1.2531e-02,  4.2195e-03,\n",
       "                      -2.8536e-02, -6.7558e-04,  1.6126e-03,  1.2878e-02, -1.6125e-03,\n",
       "                       3.6671e-02,  3.0164e-02,  6.8897e-03,  1.3704e-02, -3.4596e-03,\n",
       "                       4.1905e-02,  1.1044e-02, -9.2155e-03,  1.8147e-02, -1.3524e-02,\n",
       "                      -2.0008e-02,  4.1308e-02,  3.2912e-02,  4.2494e-02,  1.4136e-02,\n",
       "                      -1.7337e-03,  3.3653e-02,  8.4522e-03,  4.8968e-02,  1.4956e-02,\n",
       "                      -3.0139e-02,  3.3334e-02,  6.9981e-03,  3.9325e-02, -4.3525e-02,\n",
       "                       4.9671e-02, -1.2297e-02, -1.1822e-02, -4.1631e-02, -7.4125e-04,\n",
       "                      -3.3770e-02,  4.8848e-02, -1.2985e-02, -1.9473e-02,  3.2719e-02,\n",
       "                      -6.4523e-04, -3.7019e-05,  1.0071e-02, -1.2563e-02, -2.6614e-02,\n",
       "                      -3.0079e-02,  2.1721e-02, -2.4175e-02, -3.5787e-02,  4.3661e-02,\n",
       "                      -1.1809e-02,  2.9905e-02, -3.8629e-03, -3.0274e-02,  5.0413e-02,\n",
       "                      -1.2209e-02,  2.1939e-02, -2.2093e-02, -1.3030e-02,  3.6195e-02,\n",
       "                      -4.3194e-03,  4.0267e-02,  3.4687e-02,  1.5748e-02,  2.8822e-02,\n",
       "                       3.2601e-02, -4.2426e-03,  1.2343e-02,  1.1059e-02,  3.8558e-02,\n",
       "                      -3.6940e-02,  4.1703e-02, -1.2804e-02,  3.0892e-02, -4.2726e-03,\n",
       "                      -1.5837e-02,  2.0814e-02,  5.1095e-03, -1.8427e-02, -1.3863e-02,\n",
       "                      -2.3858e-02, -3.4294e-02,  4.1684e-04, -3.4113e-02,  4.6028e-02,\n",
       "                      -4.1111e-02,  8.8919e-03,  7.1893e-03,  4.5196e-02,  4.0331e-02,\n",
       "                      -2.7447e-02, -9.7043e-03, -7.7529e-03, -3.1983e-02,  3.7054e-02,\n",
       "                      -1.7185e-02, -2.1936e-02,  1.0249e-02,  2.0760e-02, -3.4663e-03,\n",
       "                      -1.7840e-02, -3.7853e-02,  3.3588e-02,  1.0655e-02,  2.1738e-03,\n",
       "                       1.2505e-02, -3.2460e-02, -7.1417e-03,  1.6379e-03, -5.0116e-03,\n",
       "                       7.2404e-03, -2.5501e-02, -3.9169e-03,  4.4146e-02,  6.4447e-03,\n",
       "                      -3.1138e-02,  1.9549e-02, -3.6894e-02,  5.0521e-02, -5.1043e-02,\n",
       "                       2.2219e-02, -1.8785e-02, -3.9274e-02, -4.0906e-02,  5.7888e-02,\n",
       "                       3.3998e-02,  2.6024e-02, -9.0811e-03,  3.9564e-02,  5.0211e-02,\n",
       "                      -3.0894e-02,  3.6990e-02,  4.8161e-03, -2.1633e-02,  2.5275e-02,\n",
       "                      -3.3299e-02,  2.2713e-02,  3.3720e-02, -3.3533e-02,  1.8522e-02,\n",
       "                      -1.9170e-02,  4.3201e-02, -4.0548e-02,  3.5176e-02,  1.7693e-02,\n",
       "                       1.4072e-02, -3.3854e-02,  8.4030e-03, -1.9256e-02,  1.5534e-02,\n",
       "                       4.0716e-02, -4.9058e-03, -1.0668e-02, -3.2230e-02,  1.6850e-02,\n",
       "                       4.9889e-02,  1.7256e-02, -2.5224e-02,  4.3273e-02, -3.1285e-02,\n",
       "                      -2.3964e-02, -2.3193e-02,  2.4125e-02, -1.1816e-02,  5.3308e-02,\n",
       "                      -1.2766e-02,  2.3926e-02,  6.1973e-03,  1.9637e-02, -2.4447e-02,\n",
       "                      -6.5046e-03,  4.1463e-02, -3.7617e-02, -3.7756e-02, -9.2886e-04,\n",
       "                      -2.6618e-02, -1.6977e-02, -2.9207e-02, -3.7497e-02, -3.1295e-02,\n",
       "                       2.2259e-02, -3.9524e-02,  2.4753e-03, -4.3995e-03, -1.9875e-02,\n",
       "                       3.8084e-02,  1.6098e-02, -2.8137e-02,  4.4069e-02,  3.9855e-04,\n",
       "                       3.5712e-02, -1.8223e-02, -6.1140e-03,  3.0147e-02,  1.6965e-02,\n",
       "                       4.1845e-02,  1.8073e-02,  1.5585e-02,  3.5646e-02, -4.1974e-04,\n",
       "                      -1.3199e-02, -3.9236e-03, -2.9836e-03,  1.4671e-02,  1.0255e-02,\n",
       "                       3.7658e-02,  2.9640e-02,  3.6022e-02,  1.6783e-02,  3.0413e-02,\n",
       "                      -3.0137e-02,  3.5007e-02, -2.3021e-02,  3.1590e-02,  1.0660e-02,\n",
       "                      -4.6116e-02,  1.8551e-02, -2.9864e-02, -4.0196e-02, -4.3147e-02,\n",
       "                       1.2411e-02,  5.6649e-03, -1.2651e-02, -4.6119e-02,  2.9566e-02,\n",
       "                      -3.4804e-02, -3.7632e-02,  3.7666e-02, -3.2591e-02, -1.1087e-02,\n",
       "                       5.7961e-02,  1.5075e-02,  4.0585e-02,  3.1886e-02, -1.1683e-02,\n",
       "                       1.9221e-02, -9.5105e-03, -3.3161e-02,  3.0849e-03,  1.1161e-02,\n",
       "                       5.3588e-02,  1.6718e-02,  1.1429e-02,  4.2702e-02,  5.0143e-02,\n",
       "                       2.5339e-02,  1.9239e-02, -2.9327e-02,  4.8707e-02,  1.7949e-02,\n",
       "                      -3.3553e-03, -6.2744e-03,  1.4927e-02, -2.5006e-03, -1.3638e-02,\n",
       "                      -4.5330e-02, -3.0686e-02, -2.7750e-02,  1.7375e-03, -2.1112e-02,\n",
       "                      -3.8948e-02,  3.0836e-02,  1.8373e-02,  1.7148e-02,  1.3584e-02,\n",
       "                       1.3244e-02,  2.4570e-02, -7.8917e-03, -3.7429e-03, -3.6154e-03,\n",
       "                      -3.5606e-02, -3.8262e-02,  3.1504e-02,  3.0589e-02, -2.1091e-02,\n",
       "                      -1.3015e-02,  1.7871e-02,  1.7900e-02,  6.1541e-03, -4.1531e-03,\n",
       "                      -6.3942e-03,  1.0999e-02,  3.7548e-02, -1.3131e-02,  4.5958e-03,\n",
       "                       2.8320e-02, -1.9961e-02,  5.2099e-03, -2.4669e-02, -1.0852e-03,\n",
       "                       3.2716e-02,  3.9475e-02, -2.7877e-02,  2.5066e-03, -3.0674e-02,\n",
       "                       2.0394e-02, -3.9077e-02, -3.1819e-02,  1.4845e-02, -2.9234e-02,\n",
       "                       2.8935e-02, -9.4941e-03, -3.5855e-02,  3.9127e-02, -1.2285e-02,\n",
       "                      -3.3727e-02,  2.9197e-02, -3.2982e-02, -5.2156e-03,  8.5077e-03,\n",
       "                      -1.9481e-03, -2.2484e-02,  8.4704e-03,  1.7211e-02,  8.3171e-03,\n",
       "                      -1.9641e-02, -4.3654e-02,  4.5077e-02, -3.4153e-02,  1.9193e-02,\n",
       "                      -3.5656e-02,  6.2287e-02,  3.6825e-02,  3.1422e-02,  2.6642e-02,\n",
       "                       5.6673e-03,  2.0726e-02, -3.4774e-02,  3.2979e-02, -1.3724e-02,\n",
       "                       4.9892e-03, -2.0926e-02,  4.8241e-02,  3.5273e-03, -3.0398e-03,\n",
       "                       3.6474e-02,  2.4151e-02, -9.0530e-03, -2.7776e-02,  3.1043e-02,\n",
       "                       5.0310e-02, -3.8842e-02, -2.6435e-02,  2.3419e-02,  1.2005e-02,\n",
       "                      -2.6487e-02,  2.4436e-02, -3.3121e-02, -9.3983e-03, -2.2005e-02,\n",
       "                       1.0711e-02,  3.4391e-02,  2.8622e-02, -2.0109e-04,  3.5667e-02,\n",
       "                      -2.9609e-02,  4.5712e-02,  3.8152e-03,  2.8290e-02, -2.2399e-02,\n",
       "                      -2.8355e-04,  7.5416e-03,  3.7170e-02, -2.1127e-02, -3.6609e-02,\n",
       "                      -4.2824e-02, -3.0331e-02,  2.7890e-02,  3.5136e-02, -2.3447e-03,\n",
       "                       2.9236e-02, -1.1555e-02, -1.9532e-02, -1.9620e-02,  5.6985e-03,\n",
       "                       3.8541e-02,  1.1786e-02, -4.2564e-02,  2.5765e-02,  4.6645e-02,\n",
       "                       5.3940e-03,  8.8965e-03,  1.7274e-02,  8.7884e-03,  4.0901e-02,\n",
       "                       3.3013e-02, -3.4354e-02, -3.9235e-02,  1.3977e-02,  2.9945e-02,\n",
       "                       4.7718e-02, -6.3605e-03,  4.7337e-02, -1.8434e-02, -3.2740e-02,\n",
       "                      -1.2858e-02,  4.6373e-02, -2.9689e-02,  3.1358e-02, -1.1587e-02,\n",
       "                      -1.0341e-02, -2.2188e-03,  4.1221e-02, -2.3929e-02, -1.7074e-03,\n",
       "                       7.9953e-03, -2.4334e-02,  1.9693e-02,  1.9499e-02, -1.0624e-02,\n",
       "                      -1.4294e-02, -3.5385e-02, -4.2131e-02, -3.6141e-02,  1.4975e-02,\n",
       "                      -2.2961e-02,  3.5967e-02,  1.1810e-02, -4.5547e-04,  3.7581e-02,\n",
       "                       3.5785e-02,  2.7864e-03,  2.0949e-03,  2.6302e-03, -1.9041e-02,\n",
       "                       1.2778e-02, -7.1364e-03,  2.0812e-02,  1.9460e-02,  2.0049e-02,\n",
       "                       9.6576e-03, -3.3192e-02,  3.1452e-02, -1.5617e-02,  1.0366e-02,\n",
       "                      -5.9694e-04, -1.2208e-02, -2.9656e-02, -2.6184e-02, -7.7300e-03,\n",
       "                       4.5543e-02,  2.1779e-02,  4.9391e-02, -4.6642e-02, -3.1766e-02,\n",
       "                       4.3159e-02, -2.4183e-02, -9.0598e-03,  8.7882e-03, -4.8265e-04,\n",
       "                      -2.5387e-02, -4.0357e-02,  1.4505e-02,  2.3080e-02,  2.9450e-02,\n",
       "                       2.5248e-02, -2.0265e-02, -1.4880e-02, -3.9304e-03,  2.1978e-02,\n",
       "                      -1.0536e-02,  2.8778e-02,  3.8502e-02, -3.2649e-02,  1.1938e-02,\n",
       "                       1.0632e-02,  3.0907e-02,  2.5431e-02,  2.4099e-02, -4.2344e-03,\n",
       "                      -2.6769e-02, -8.7144e-03,  1.1013e-02,  3.1954e-02, -3.7295e-02,\n",
       "                      -3.2197e-02,  2.0777e-02,  2.6208e-02, -3.9630e-02, -8.8997e-03,\n",
       "                      -2.1126e-02,  5.8213e-03, -1.6031e-02,  4.2549e-02,  2.2548e-02,\n",
       "                      -2.7258e-02,  3.0966e-02, -5.7899e-03,  3.8986e-02,  2.9044e-02,\n",
       "                      -2.6768e-02,  8.7581e-04,  1.9186e-02,  5.2233e-02,  1.1948e-02,\n",
       "                       4.1519e-02,  1.9655e-02])),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[-0.0480,  0.0241, -0.0603,  ..., -0.0221, -0.0458,  0.0118],\n",
       "                      [ 0.0404,  0.0210,  0.0879,  ...,  0.0130, -0.0201,  0.0056],\n",
       "                      [ 0.0156, -0.0260, -0.0410,  ..., -0.0544,  0.0482, -0.0134],\n",
       "                      ...,\n",
       "                      [ 0.0248, -0.0423,  0.0033,  ...,  0.0120,  0.0194,  0.0773],\n",
       "                      [ 0.0164, -0.0127,  0.0075,  ...,  0.0713, -0.0247, -0.0106],\n",
       "                      [-0.0117,  0.0097, -0.0150,  ..., -0.0226,  0.0662,  0.0451]])),\n",
       "             ('linear_relu_stack.4.bias',\n",
       "              tensor([-0.0349, -0.0152, -0.0437,  0.0265, -0.0911,  0.1923, -0.0211,  0.0725,\n",
       "                      -0.0681, -0.0091]))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be588346",
   "metadata": {},
   "source": [
    "**Save model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f3de8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'basic_nn_fashion_mnist.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432bb206",
   "metadata": {},
   "source": [
    "**Load Mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9b6b708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('basic_nn_fashion_mnist.pth', weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8dd114",
   "metadata": {},
   "source": [
    "**Save Architecture + Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af0e812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'basic_nn_fashion_mnist_architecture.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676160ed",
   "metadata": {},
   "source": [
    "**Load Architecture + Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12fa61f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('basic_nn_fashion_mnist_architecture.pth', weights_only=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61dd248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

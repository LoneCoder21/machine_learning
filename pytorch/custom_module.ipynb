{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bdc0473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e8a741",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e38bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b4d77c",
   "metadata": {},
   "source": [
    "**Custom Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68314745",
   "metadata": {},
   "outputs": [],
   "source": [
    "class c_Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        std_dev = torch.sqrt(1 / torch.tensor(in_features))\n",
    "        self.weights = nn.Parameter(torch.randn((in_features, out_features)) * std_dev)\n",
    "        self.bias = nn.Parameter(torch.randn((1, out_features)) * std_dev)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x @ self.weights\n",
    "        x = x + self.bias\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3017d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class c_ReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.maximum(x, torch.tensor(0, device=x.device))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5dfd7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): c_Linear()\n",
       "    (1): c_ReLU()\n",
       "    (2): c_Linear()\n",
       "    (3): c_ReLU()\n",
       "    (4): c_Linear()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            c_Linear(28*28, 512),\n",
    "            c_ReLU(),\n",
    "            c_Linear(512, 512),\n",
    "            c_ReLU(),\n",
    "            c_Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75cb02b",
   "metadata": {},
   "source": [
    "**Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d6113ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbdf633",
   "metadata": {},
   "source": [
    "**Loss Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f4ffbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b3483",
   "metadata": {},
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "238552fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352c73b9",
   "metadata": {},
   "source": [
    "**Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc2f1135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3770b5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.283262  [   64/60000]\n",
      "loss: 2.191972  [ 6464/60000]\n",
      "loss: 2.038403  [12864/60000]\n",
      "loss: 2.013173  [19264/60000]\n",
      "loss: 1.895012  [25664/60000]\n",
      "loss: 1.787168  [32064/60000]\n",
      "loss: 1.755145  [38464/60000]\n",
      "loss: 1.604513  [44864/60000]\n",
      "loss: 1.579576  [51264/60000]\n",
      "loss: 1.456357  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 1.459977 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.498345  [   64/60000]\n",
      "loss: 1.475300  [ 6464/60000]\n",
      "loss: 1.259487  [12864/60000]\n",
      "loss: 1.362056  [19264/60000]\n",
      "loss: 1.207773  [25664/60000]\n",
      "loss: 1.188057  [32064/60000]\n",
      "loss: 1.194746  [38464/60000]\n",
      "loss: 1.106400  [44864/60000]\n",
      "loss: 1.125696  [51264/60000]\n",
      "loss: 1.032214  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 1.057028 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.090392  [   64/60000]\n",
      "loss: 1.125175  [ 6464/60000]\n",
      "loss: 0.897557  [12864/60000]\n",
      "loss: 1.074420  [19264/60000]\n",
      "loss: 0.939251  [25664/60000]\n",
      "loss: 0.942777  [32064/60000]\n",
      "loss: 0.980793  [38464/60000]\n",
      "loss: 0.915232  [44864/60000]\n",
      "loss: 0.944128  [51264/60000]\n",
      "loss: 0.874135  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.892190 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.897810  [   64/60000]\n",
      "loss: 0.973130  [ 6464/60000]\n",
      "loss: 0.733205  [12864/60000]\n",
      "loss: 0.943847  [19264/60000]\n",
      "loss: 0.827157  [25664/60000]\n",
      "loss: 0.824280  [32064/60000]\n",
      "loss: 0.880541  [38464/60000]\n",
      "loss: 0.826943  [44864/60000]\n",
      "loss: 0.852652  [51264/60000]\n",
      "loss: 0.792431  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.804802 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.787034  [   64/60000]\n",
      "loss: 0.887257  [ 6464/60000]\n",
      "loss: 0.640980  [12864/60000]\n",
      "loss: 0.869450  [19264/60000]\n",
      "loss: 0.765285  [25664/60000]\n",
      "loss: 0.752045  [32064/60000]\n",
      "loss: 0.817952  [38464/60000]\n",
      "loss: 0.775883  [44864/60000]\n",
      "loss: 0.795488  [51264/60000]\n",
      "loss: 0.737130  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.747461 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c89b207e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weights',\n",
       "              tensor([[-0.0818, -0.0226,  0.0455,  ..., -0.0118, -0.0375, -0.0901],\n",
       "                      [ 0.0249, -0.0368,  0.0305,  ..., -0.0026,  0.0490, -0.0085],\n",
       "                      [ 0.0013,  0.0122,  0.0409,  ...,  0.0526, -0.0342,  0.0186],\n",
       "                      ...,\n",
       "                      [-0.0565, -0.0290, -0.0123,  ..., -0.0072, -0.0196,  0.0396],\n",
       "                      [ 0.0153, -0.0136,  0.0315,  ...,  0.0650,  0.0906, -0.0489],\n",
       "                      [ 0.0537, -0.0432, -0.0133,  ..., -0.0268, -0.0254,  0.0125]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([[ 7.9588e-04, -1.1547e-02, -4.9352e-02,  4.5235e-03,  6.0220e-02,\n",
       "                        7.7197e-03, -3.2454e-02, -2.7994e-02, -3.9177e-02, -1.0494e-02,\n",
       "                        1.0827e-02,  2.3062e-02,  2.3528e-02,  1.2449e-02, -1.4877e-02,\n",
       "                        4.0454e-03,  6.6616e-02,  2.4341e-02,  1.4857e-02, -1.7112e-02,\n",
       "                        4.4628e-03, -9.8213e-03,  2.4468e-02,  1.5320e-02, -1.4515e-02,\n",
       "                       -2.2723e-02, -5.6606e-02,  3.7325e-02, -2.0167e-02, -3.1355e-02,\n",
       "                       -5.0592e-03,  7.8376e-02, -5.0989e-04, -6.2870e-02, -2.7512e-02,\n",
       "                        1.4470e-02,  2.3720e-02, -1.2527e-02, -3.2503e-02,  9.9692e-03,\n",
       "                       -5.2623e-02, -1.5449e-02,  9.8406e-03,  3.3746e-02,  4.2727e-02,\n",
       "                       -1.0439e-02, -2.9606e-02, -1.6905e-02,  6.6265e-02,  4.5085e-02,\n",
       "                        3.1999e-02, -1.7084e-02,  2.3756e-02, -5.1721e-02, -2.2570e-04,\n",
       "                       -2.8705e-02,  1.8663e-02, -5.7615e-03,  2.8210e-03, -5.1085e-02,\n",
       "                       -1.7716e-02, -3.0913e-02,  6.9062e-04,  3.6298e-03, -1.0158e-02,\n",
       "                        1.1503e-03,  2.6154e-02,  3.3645e-02, -7.0443e-03, -2.1768e-02,\n",
       "                        2.1658e-03, -3.5662e-02, -6.1591e-03, -5.4883e-02,  5.4294e-02,\n",
       "                       -1.1829e-02,  3.2548e-02, -7.8682e-02, -4.7632e-02,  5.6532e-02,\n",
       "                       -3.5653e-02,  4.3026e-03,  2.5170e-02,  3.3410e-02,  5.2005e-02,\n",
       "                       -3.4925e-02,  4.5228e-02,  4.2849e-02, -9.6257e-03,  3.4501e-02,\n",
       "                        9.6324e-03, -1.1844e-02, -7.7625e-03, -1.0375e-03,  6.3400e-03,\n",
       "                       -5.5429e-02,  6.8335e-02, -4.2431e-03,  5.4318e-02, -6.0400e-02,\n",
       "                       -4.4798e-02, -1.8138e-03,  1.6133e-02, -7.6689e-02,  1.6506e-02,\n",
       "                        4.5223e-02,  6.5885e-03, -3.8036e-03,  3.8527e-03, -5.9780e-03,\n",
       "                       -1.4972e-02,  1.2775e-02,  4.9971e-03,  4.4730e-02,  6.9062e-03,\n",
       "                        2.2998e-04, -1.5736e-02,  2.1018e-02,  1.3906e-02, -4.8419e-02,\n",
       "                       -2.0645e-02, -6.8314e-03, -4.6878e-02, -9.7921e-02,  8.7016e-02,\n",
       "                       -5.2124e-02, -1.9459e-02, -4.8772e-02, -3.8912e-02, -9.0308e-02,\n",
       "                       -1.7106e-02, -2.5282e-02,  3.8193e-02,  2.5211e-02, -5.3319e-02,\n",
       "                       -1.0563e-02, -8.6542e-03,  7.1859e-02,  1.5220e-02, -8.5607e-02,\n",
       "                       -2.6781e-02, -2.1362e-02,  3.9062e-02,  3.3963e-02, -5.1632e-02,\n",
       "                       -4.4856e-03,  8.5776e-03, -3.5377e-02,  1.9674e-02, -4.0550e-02,\n",
       "                       -2.0904e-02, -9.5519e-04, -4.4965e-02,  1.9848e-02, -2.3477e-02,\n",
       "                        1.3596e-02, -1.8125e-02, -8.3150e-03, -1.8681e-02,  5.4714e-02,\n",
       "                        8.8582e-03,  4.6268e-02, -1.3184e-02, -7.4057e-03, -1.2072e-02,\n",
       "                        3.7598e-02,  5.1140e-02, -4.1454e-02,  7.9526e-03, -3.1690e-03,\n",
       "                        2.7437e-02, -3.4823e-02,  4.2970e-02, -2.5927e-02,  3.8765e-02,\n",
       "                       -1.3045e-02, -5.4499e-02,  5.5398e-02,  2.7325e-02, -2.6046e-02,\n",
       "                        7.1509e-02, -4.2488e-02,  3.7296e-04, -3.3560e-02,  9.1479e-02,\n",
       "                        3.8964e-02, -6.8648e-03,  1.0121e-02, -1.5508e-02,  8.4063e-02,\n",
       "                       -1.6096e-02,  2.2340e-02, -4.7647e-04,  4.9145e-02,  4.2806e-02,\n",
       "                       -1.3996e-02, -3.5008e-02,  8.6259e-02,  4.4098e-02, -1.9460e-02,\n",
       "                        4.1593e-02, -9.3561e-03, -1.3196e-02, -4.3897e-02, -9.1299e-02,\n",
       "                        9.5063e-03,  1.9534e-02, -1.1832e-02,  1.2501e-02, -2.7365e-02,\n",
       "                       -4.1993e-03, -5.0610e-02, -8.1206e-02,  3.9040e-02, -2.9079e-02,\n",
       "                        6.6658e-03,  6.7318e-02, -3.8855e-04, -7.8999e-03, -2.3552e-02,\n",
       "                       -1.2665e-02,  2.2132e-02,  7.1406e-02,  1.7011e-02, -1.6337e-02,\n",
       "                        2.7161e-02,  3.4497e-02,  5.8787e-02,  6.5751e-03, -1.5601e-02,\n",
       "                        3.3650e-02, -3.6714e-02,  3.9290e-02, -2.4863e-02, -5.1197e-02,\n",
       "                        1.2224e-02, -1.8254e-02,  3.4502e-02, -3.1219e-02,  1.6063e-02,\n",
       "                       -5.4311e-02,  3.7922e-03, -1.0342e-02,  4.2943e-02,  3.1967e-02,\n",
       "                       -1.7522e-02, -2.2833e-02, -2.4991e-02, -4.4428e-02,  4.2871e-03,\n",
       "                        4.6814e-02,  1.1683e-02, -2.9590e-02, -1.7367e-02, -3.6472e-02,\n",
       "                        1.5707e-02, -1.4383e-02,  1.5601e-02,  1.9921e-02,  5.9373e-02,\n",
       "                       -3.5041e-02,  8.8495e-02, -3.2047e-02,  7.5874e-03, -1.7102e-02,\n",
       "                       -4.2116e-02, -4.4564e-02,  5.6749e-03, -1.3560e-02, -2.2388e-02,\n",
       "                        6.5864e-04, -4.3884e-02, -2.1766e-02, -1.4789e-02, -7.2424e-02,\n",
       "                        3.9309e-02,  2.9216e-02,  7.0608e-03, -1.1190e-02, -4.5384e-03,\n",
       "                        4.2569e-02,  8.9267e-05,  5.5322e-03,  4.1506e-02, -1.4679e-02,\n",
       "                       -1.2854e-02, -3.0222e-02,  6.9419e-03, -5.7815e-02,  3.0523e-02,\n",
       "                        2.4844e-02,  8.5445e-03, -7.0267e-02,  3.1187e-03,  3.4800e-02,\n",
       "                        9.6521e-04,  2.9965e-02,  4.6676e-03, -6.2213e-02, -1.1570e-02,\n",
       "                       -5.5420e-02,  6.3795e-02, -1.6139e-03, -3.7131e-02,  3.8548e-02,\n",
       "                        1.2574e-02, -2.2247e-02, -1.5569e-02, -2.2454e-02, -2.0637e-02,\n",
       "                        3.5088e-02, -8.4787e-03,  2.6551e-04, -3.7140e-02,  2.3753e-02,\n",
       "                       -2.0658e-02, -3.4737e-02,  1.9216e-02, -4.7744e-02, -4.5009e-03,\n",
       "                       -8.8766e-03, -3.5053e-02, -1.0868e-02, -6.0334e-02, -5.4194e-03,\n",
       "                        2.2433e-02,  2.4778e-02,  2.2658e-02,  3.3883e-02,  4.2893e-02,\n",
       "                       -4.4580e-02, -2.1490e-02,  4.0001e-02,  1.6670e-02, -2.0483e-02,\n",
       "                        8.9979e-03,  2.8429e-02,  3.0091e-02,  1.7407e-02, -1.8230e-02,\n",
       "                       -3.9097e-03,  5.4325e-02,  9.7698e-02, -1.0987e-02,  4.3243e-02,\n",
       "                       -3.4461e-02,  2.3836e-02, -5.7356e-02,  3.7987e-02, -3.6776e-02,\n",
       "                       -5.5515e-02,  3.3515e-02, -2.0106e-02,  3.1930e-02, -3.3267e-02,\n",
       "                        5.2256e-02, -8.5533e-03, -5.1908e-03, -2.7254e-02, -6.6856e-03,\n",
       "                        2.0278e-02, -5.8776e-03,  8.1870e-02, -1.3607e-02,  6.4180e-02,\n",
       "                        3.7229e-02,  7.8500e-02,  1.9092e-02,  6.3931e-02,  5.5204e-02,\n",
       "                       -1.0687e-02,  8.9951e-04,  4.6119e-02, -4.6750e-02, -5.3288e-03,\n",
       "                        1.3056e-02,  7.1074e-02,  4.7099e-02, -7.3471e-02, -2.2380e-02,\n",
       "                       -5.1508e-02, -3.5626e-03,  4.7390e-03,  5.2422e-02, -2.7747e-02,\n",
       "                        4.2091e-02,  1.8688e-02,  1.0432e-02, -2.6716e-02,  9.0788e-03,\n",
       "                       -2.2311e-02, -1.2244e-02,  2.3415e-02,  2.0571e-02, -3.6045e-02,\n",
       "                       -3.6907e-03, -8.3503e-03, -4.2211e-02, -1.5798e-02,  3.4978e-02,\n",
       "                        1.0685e-02, -2.7291e-02,  5.1783e-02,  3.8754e-02, -1.1911e-02,\n",
       "                       -2.4431e-02,  1.3651e-02,  1.3297e-02,  3.6273e-03,  5.3455e-02,\n",
       "                       -5.1681e-02, -3.6264e-02,  4.3926e-02, -3.0258e-02,  4.3417e-03,\n",
       "                        4.7720e-02, -3.8156e-03, -4.3960e-02, -2.1206e-02,  2.1223e-02,\n",
       "                       -1.3966e-02,  2.1639e-02, -3.8766e-02, -2.4782e-02,  6.0775e-02,\n",
       "                        7.2057e-03,  3.0370e-03,  2.4988e-02, -4.3030e-02,  6.8687e-02,\n",
       "                        1.5946e-02, -7.2005e-02,  2.2761e-02,  3.5641e-02,  1.1354e-02,\n",
       "                        4.2658e-02, -2.9539e-02,  1.8690e-02,  2.1440e-02,  4.4756e-03,\n",
       "                       -7.5617e-04, -4.1800e-02, -6.3607e-02,  7.8887e-03,  2.4200e-02,\n",
       "                       -6.5386e-03, -3.8182e-02, -1.9835e-02, -6.9304e-02,  5.0468e-02,\n",
       "                       -2.2595e-02,  1.6528e-03, -2.4295e-02,  5.6851e-02, -7.5060e-02,\n",
       "                        7.3234e-02,  3.8393e-03,  2.9979e-02, -1.7490e-03, -6.4907e-02,\n",
       "                        2.4278e-02,  3.0939e-02, -1.5977e-02,  4.3598e-03,  4.2137e-02,\n",
       "                        1.3543e-02,  2.3152e-02,  7.5149e-03,  1.9441e-02, -8.4104e-04,\n",
       "                        2.8538e-02,  1.9236e-02, -3.5845e-02, -3.7751e-02,  3.2262e-03,\n",
       "                        3.1284e-02,  2.9285e-02, -1.4896e-02, -2.9717e-02, -4.4949e-02,\n",
       "                        1.5280e-02,  5.1820e-03, -3.1200e-02,  3.4907e-02, -6.8560e-02,\n",
       "                        4.6419e-03, -2.9404e-02,  2.6988e-02, -4.7479e-03,  7.4322e-02,\n",
       "                        8.6544e-02,  4.0933e-02, -7.4536e-02, -1.3275e-02,  2.4578e-02,\n",
       "                        1.8384e-02, -6.6537e-02, -1.1584e-02, -4.9791e-02,  2.6003e-02,\n",
       "                        6.0338e-02, -8.0511e-02, -3.1737e-03, -1.0678e-01, -6.6379e-03,\n",
       "                        5.6324e-02,  9.1580e-03, -1.0692e-02, -4.0760e-02, -4.8286e-02,\n",
       "                       -4.7718e-02, -2.0974e-02]])),\n",
       "             ('linear_relu_stack.2.weights',\n",
       "              tensor([[ 0.0130, -0.0435,  0.0444,  ..., -0.0009, -0.0226, -0.0173],\n",
       "                      [ 0.0307, -0.0061,  0.0142,  ..., -0.0381, -0.0254,  0.0541],\n",
       "                      [-0.0027,  0.0775, -0.0035,  ...,  0.0224, -0.0272, -0.0257],\n",
       "                      ...,\n",
       "                      [-0.0386,  0.0097, -0.0257,  ...,  0.0199, -0.0496,  0.0138],\n",
       "                      [-0.0970,  0.0308,  0.0348,  ..., -0.0354, -0.0529,  0.0334],\n",
       "                      [ 0.0195,  0.0707,  0.0462,  ...,  0.0028, -0.0599, -0.0260]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([[ 0.0055, -0.0078, -0.0141,  0.0614,  0.0020,  0.0227,  0.0188, -0.0716,\n",
       "                        0.0092, -0.0522, -0.0432, -0.0101, -0.0253,  0.0758, -0.0437, -0.0006,\n",
       "                       -0.0185, -0.0308,  0.0889, -0.0228,  0.0780, -0.0339, -0.0253, -0.0413,\n",
       "                       -0.0168,  0.0035,  0.0115,  0.0486, -0.0619, -0.0025, -0.0390,  0.0030,\n",
       "                        0.0193,  0.0218,  0.0035, -0.1127,  0.0023, -0.0610,  0.0060, -0.0254,\n",
       "                        0.0095,  0.0539, -0.0273, -0.0193,  0.0023, -0.0712,  0.0032,  0.0560,\n",
       "                       -0.0221,  0.0318,  0.0103, -0.0607,  0.0384, -0.0070,  0.0119,  0.0869,\n",
       "                        0.0600,  0.0679, -0.0105, -0.0344,  0.0226,  0.0263, -0.0194, -0.0404,\n",
       "                        0.0919,  0.0130,  0.0007,  0.0084,  0.0072,  0.0140, -0.0176,  0.0081,\n",
       "                       -0.0570, -0.0610, -0.0477,  0.0339,  0.0638, -0.0009, -0.0081, -0.0492,\n",
       "                       -0.0638, -0.0063,  0.0618, -0.0100,  0.0057, -0.0336, -0.0680, -0.0091,\n",
       "                        0.0049,  0.0622, -0.0227, -0.0679,  0.0027,  0.0128, -0.0796,  0.0369,\n",
       "                        0.0576,  0.0011,  0.0203,  0.0605, -0.0026,  0.0851, -0.0730,  0.0395,\n",
       "                       -0.0419,  0.0269, -0.0267,  0.0328, -0.0164, -0.0375, -0.0729,  0.0456,\n",
       "                       -0.0534, -0.0028,  0.0317,  0.0313, -0.0047, -0.0683, -0.0175,  0.0099,\n",
       "                        0.0429,  0.0963, -0.0867,  0.0246,  0.0059, -0.0508,  0.1014,  0.0128,\n",
       "                        0.0595, -0.0079,  0.0189,  0.0341,  0.0206,  0.0085,  0.0237, -0.0477,\n",
       "                        0.0133, -0.0354,  0.0358, -0.0349, -0.0360,  0.0462,  0.0060,  0.0045,\n",
       "                       -0.0739,  0.0248, -0.0831, -0.0044, -0.0164,  0.0031,  0.0709, -0.0069,\n",
       "                       -0.0116,  0.0179, -0.0169, -0.0013,  0.0292,  0.0451, -0.0076,  0.0616,\n",
       "                       -0.0423,  0.0204, -0.0792,  0.0059,  0.0763, -0.0026, -0.0276,  0.0501,\n",
       "                        0.0053, -0.0152,  0.0306,  0.0698,  0.0919,  0.0087,  0.0830, -0.0204,\n",
       "                        0.0334,  0.0524, -0.0210, -0.0282, -0.0180, -0.0003,  0.0698, -0.0946,\n",
       "                       -0.0383,  0.0041,  0.0776,  0.0518,  0.0079,  0.0035,  0.0560, -0.0940,\n",
       "                        0.0430, -0.0165,  0.0092,  0.0579,  0.0601,  0.0122,  0.0836, -0.0041,\n",
       "                       -0.0005,  0.0322, -0.0728, -0.0273, -0.0061,  0.0685,  0.0636, -0.0255,\n",
       "                        0.0246, -0.0322,  0.0041, -0.0670, -0.0671,  0.0088, -0.0110,  0.0231,\n",
       "                        0.0133, -0.0996, -0.0432,  0.0316, -0.0522,  0.0312, -0.0294, -0.0214,\n",
       "                        0.0291,  0.0344,  0.0234, -0.0273,  0.0408,  0.0577, -0.0014, -0.0540,\n",
       "                       -0.0756,  0.0913, -0.0301,  0.0151,  0.0033,  0.0343, -0.0490, -0.0201,\n",
       "                       -0.0033,  0.0018, -0.0346,  0.0330,  0.0642,  0.0314, -0.0345,  0.0507,\n",
       "                        0.0243, -0.0097, -0.0179, -0.0211,  0.0210, -0.0741,  0.0031, -0.0182,\n",
       "                       -0.0207,  0.0600, -0.0415, -0.0577,  0.0555, -0.0139,  0.0180, -0.0756,\n",
       "                        0.0260,  0.0183, -0.0011, -0.0247,  0.0072,  0.0182,  0.0685, -0.0442,\n",
       "                        0.0296, -0.0024,  0.0060,  0.0479, -0.0505,  0.0358, -0.1179,  0.0017,\n",
       "                        0.0349,  0.0556, -0.0031,  0.0465, -0.0018,  0.0525, -0.0557,  0.0424,\n",
       "                        0.0457, -0.0137,  0.0313,  0.0061, -0.0168,  0.0165, -0.0281, -0.0087,\n",
       "                       -0.0371, -0.0277, -0.0103,  0.0101, -0.0394,  0.0579, -0.0493,  0.0077,\n",
       "                        0.0540,  0.0017, -0.0177,  0.0593, -0.0302,  0.1072,  0.1155,  0.1088,\n",
       "                       -0.0197,  0.0716,  0.0400, -0.0083,  0.0343,  0.0121, -0.0506, -0.0060,\n",
       "                       -0.0052, -0.0832, -0.0096, -0.0704,  0.0577,  0.0193, -0.0419, -0.0110,\n",
       "                       -0.0439, -0.0153, -0.0034,  0.0634,  0.0575, -0.0165,  0.0209,  0.0511,\n",
       "                       -0.0017,  0.0802,  0.0638,  0.0041, -0.0561, -0.0366, -0.0146, -0.0484,\n",
       "                       -0.0266,  0.0054, -0.0394, -0.0283, -0.0205, -0.0126, -0.0188,  0.0261,\n",
       "                       -0.0298, -0.0565,  0.0119, -0.0694,  0.0525,  0.0395, -0.0247,  0.0038,\n",
       "                       -0.0021,  0.0270, -0.0237, -0.0496, -0.0527,  0.1311, -0.0568, -0.0194,\n",
       "                       -0.0156, -0.0338, -0.0449,  0.0294, -0.0216, -0.0700,  0.0593, -0.0104,\n",
       "                       -0.0269,  0.0334,  0.0126, -0.0326, -0.0577, -0.0362, -0.0905, -0.0837,\n",
       "                        0.0356, -0.0354, -0.0369,  0.1085,  0.0386, -0.0046,  0.0561,  0.0597,\n",
       "                       -0.1160, -0.0041, -0.0146,  0.0367,  0.1128, -0.0165, -0.0204, -0.0052,\n",
       "                       -0.0141, -0.1101, -0.0314,  0.0290, -0.0637,  0.0095, -0.0639,  0.0316,\n",
       "                       -0.0317, -0.0056, -0.0004, -0.0404, -0.0335, -0.0209, -0.0463,  0.0356,\n",
       "                        0.0395,  0.0256,  0.0821,  0.0033, -0.0138, -0.0276,  0.0523, -0.0050,\n",
       "                        0.0339, -0.0271, -0.0169,  0.0317,  0.0532, -0.0794, -0.0199,  0.0115,\n",
       "                       -0.0095,  0.0514,  0.0004,  0.0142,  0.0371,  0.0637, -0.0582,  0.0264,\n",
       "                       -0.0049, -0.0069,  0.1034,  0.0318,  0.0306,  0.0198,  0.0057, -0.0516,\n",
       "                        0.0564,  0.0149, -0.0259, -0.0148,  0.0371, -0.0231,  0.0601,  0.0034,\n",
       "                        0.0299,  0.0345, -0.0504,  0.0524, -0.0599,  0.0095,  0.0005,  0.0644,\n",
       "                       -0.0180,  0.0031,  0.0019,  0.0476,  0.0397,  0.0223,  0.0766,  0.0699,\n",
       "                       -0.0377, -0.0334,  0.0355, -0.0190, -0.0404,  0.0565, -0.0406, -0.0046,\n",
       "                        0.0216,  0.0751,  0.0222,  0.0653, -0.0146, -0.0627,  0.0054, -0.0472,\n",
       "                       -0.0336,  0.0047, -0.0127,  0.0730, -0.0182, -0.0034,  0.0710,  0.0110,\n",
       "                        0.0195, -0.0588,  0.0125,  0.0477, -0.0573,  0.0595, -0.0444, -0.0074,\n",
       "                        0.0028,  0.0151,  0.0254, -0.0267, -0.0104,  0.0013, -0.0139,  0.0058]])),\n",
       "             ('linear_relu_stack.4.weights',\n",
       "              tensor([[ 0.0230, -0.0553,  0.0511,  ..., -0.0211, -0.0209, -0.1135],\n",
       "                      [ 0.0066,  0.0217,  0.0970,  ..., -0.0532,  0.0228, -0.0432],\n",
       "                      [ 0.0162,  0.0135, -0.0276,  ..., -0.0580, -0.0072,  0.0198],\n",
       "                      ...,\n",
       "                      [-0.0224,  0.1478,  0.0317,  ..., -0.0326,  0.0531, -0.0259],\n",
       "                      [ 0.0582,  0.0593, -0.0509,  ..., -0.0002, -0.0179, -0.0318],\n",
       "                      [-0.0747, -0.1177,  0.0067,  ..., -0.0635,  0.0380, -0.1514]])),\n",
       "             ('linear_relu_stack.4.bias',\n",
       "              tensor([[ 0.0102,  0.0295, -0.0186,  0.0352, -0.0324,  0.1432,  0.0502,  0.0662,\n",
       "                       -0.0377,  0.0111]]))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be588346",
   "metadata": {},
   "source": [
    "**Save model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f3de8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'basic_nn_fashion_mnist.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432bb206",
   "metadata": {},
   "source": [
    "**Load Mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9b6b708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): c_Linear()\n",
       "    (1): c_ReLU()\n",
       "    (2): c_Linear()\n",
       "    (3): c_ReLU()\n",
       "    (4): c_Linear()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('basic_nn_fashion_mnist.pth', weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8dd114",
   "metadata": {},
   "source": [
    "**Save Architecture + Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af0e812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'basic_nn_fashion_mnist_architecture.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676160ed",
   "metadata": {},
   "source": [
    "**Load Architecture + Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12fa61f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): c_Linear()\n",
       "    (1): c_ReLU()\n",
       "    (2): c_Linear()\n",
       "    (3): c_ReLU()\n",
       "    (4): c_Linear()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('basic_nn_fashion_mnist_architecture.pth', weights_only=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61dd248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
